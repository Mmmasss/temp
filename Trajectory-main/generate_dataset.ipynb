{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from utils import Timer\n",
    "from joblib import Parallel, delayed\n",
    "from traj2grid import Traj2Grid\n",
    "import traj_dist.distance as tdist\n",
    "from parameters import *\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data start\n",
      "read data/full/gps_20161101 done, 1.293s after read data start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2933611869812012"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer = Timer()\n",
    "\n",
    "file_path = \"data/full/gps_20161101\"\n",
    "dict_path = \"data/str_grid2idx_400_44612.json\"\n",
    "nrows = 2000000\n",
    "vocab_size = 400\n",
    "\n",
    "\n",
    "# read data\n",
    "timer.tik(\"read data\")\n",
    "df = pd.read_csv(file_path, names=[\"name\", \"id\", \"time\", \"lon\", \"lat\"],\n",
    "        usecols=[\"id\", \"time\", \"lon\", \"lat\"], nrows=nrows)\n",
    "timer.tok(\"read {}\".format(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 去除超出范围的数据点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剩1895778/2000000个点，筛掉5%\n"
     ]
    }
   ],
   "source": [
    "pad = 0.002\n",
    "\n",
    "l = len(df)\n",
    "df = df[(df[\"lon\"] > 104.04214 + pad) & (df[\"lon\"] < 104.12958 - pad)]\n",
    "df = df[(df[\"lat\"] > 30.65294 + pad) & (df[\"lat\"] < 30.72775 - pad)]\n",
    "print(f\"剩{len(df)}/{l}个点，筛掉{round(100 - 100 * len(df) / l)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy转换为1维点列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dictdata/str_grid2idx_400_44612.json done, 7.918s after read data start\n",
      "group-apply done, 109.802s after read data start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109.80250597000122"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_grid2idx = json.load(open(dict_path))\n",
    "t2g = Traj2Grid(row_num, column_num, min_lon, min_lat, max_lon, max_lat)\n",
    "grid2idx = {eval(g): str_grid2idx[g] for g in list(str_grid2idx)}\n",
    "t2g.set_vocab(grid2idx)\n",
    "timer.tok(f\"load dict{dict_path}\")\n",
    "\n",
    "\n",
    "def group_concat(group: pd.DataFrame):\n",
    "    origin_traj = [((row[\"lon\"]), row[\"lat\"])\n",
    "                   for index, row in group.iterrows()]\n",
    "    traj_1d, coord_traj = t2g.convert1d(origin_traj)\n",
    "    series = pd.Series({\n",
    "        \"origin_trajs\": coord_traj,\n",
    "        \"trajs\": traj_1d,\n",
    "        \"len\": len(traj_1d),\n",
    "        'start_time': group[\"time\"].min(),\n",
    "        'end_time': group[\"time\"].max(),\n",
    "    })\n",
    "    return series\n",
    "\n",
    "res = Parallel(n_jobs=44)(delayed(group_concat)(group)for name, group in df.groupby(\"id\"))\n",
    "df = pd.DataFrame(res)\n",
    "timer.tok(\"group-apply\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['origin_trajs'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 过滤0长度轨迹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剩10955/10955条轨迹，筛掉0%\n"
     ]
    }
   ],
   "source": [
    "dff = df[(df[\"len\"] > 0)]\n",
    "print(f\"剩{len(dff)}/{len(df)}条轨迹，筛掉{round(100 - 100 * len(dff) / len(df))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成pair-wise轨迹距离矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-0.0% done, 124.735s after read data start\n",
      "101-0.01% done, 125.2s after read data start\n",
      "201-0.03% done, 125.711s after read data start\n",
      "301-0.08% done, 127.082s after read data start\n",
      "401-0.13% done, 128.273s after read data start\n",
      "501-0.21% done, 129.803s after read data start\n",
      "601-0.3% done, 132.003s after read data start\n",
      "701-0.41% done, 134.259s after read data start\n",
      "801-0.53% done, 137.172s after read data start\n",
      "901-0.68% done, 140.11s after read data start\n",
      "1001-0.84% done, 143.655s after read data start\n",
      "1101-1.01% done, 147.629s after read data start\n",
      "1201-1.2% done, 151.902s after read data start\n",
      "1301-1.41% done, 156.656s after read data start\n",
      "1401-1.64% done, 161.546s after read data start\n",
      "1501-1.88% done, 166.763s after read data start\n",
      "1601-2.14% done, 172.515s after read data start\n",
      "1701-2.41% done, 178.446s after read data start\n",
      "1801-2.7% done, 184.638s after read data start\n",
      "1901-3.01% done, 191.668s after read data start\n",
      "2001-3.34% done, 198.653s after read data start\n",
      "2101-3.68% done, 206.638s after read data start\n",
      "2201-4.04% done, 214.41s after read data start\n",
      "2301-4.41% done, 222.833s after read data start\n",
      "2401-4.8% done, 231.642s after read data start\n",
      "2501-5.21% done, 240.452s after read data start\n",
      "2601-5.64% done, 250.065s after read data start\n",
      "2701-6.08% done, 259.815s after read data start\n",
      "2801-6.54% done, 270.036s after read data start\n",
      "2901-7.01% done, 280.435s after read data start\n",
      "3001-7.51% done, 291.882s after read data start\n",
      "3101-8.01% done, 303.167s after read data start\n",
      "3201-8.54% done, 314.565s after read data start\n",
      "3301-9.08% done, 327.031s after read data start\n",
      "3401-9.64% done, 339.29s after read data start\n",
      "3501-10.22% done, 352.403s after read data start\n",
      "3601-10.81% done, 365.276s after read data start\n",
      "3701-11.42% done, 378.96s after read data start\n",
      "3801-12.04% done, 393.463s after read data start\n",
      "3901-12.68% done, 408.535s after read data start\n",
      "4001-13.34% done, 423.126s after read data start\n",
      "4101-14.02% done, 438.534s after read data start\n",
      "4201-14.71% done, 455.415s after read data start\n",
      "4301-15.42% done, 471.81s after read data start\n",
      "4401-16.14% done, 488.664s after read data start\n",
      "4501-16.88% done, 505.709s after read data start\n",
      "4601-17.64% done, 524.333s after read data start\n",
      "4701-18.42% done, 542.469s after read data start\n",
      "4801-19.21% done, 560.877s after read data start\n",
      "4901-20.02% done, 580.159s after read data start\n",
      "5001-20.84% done, 599.924s after read data start\n",
      "5101-21.69% done, 620.09s after read data start\n",
      "5201-22.54% done, 640.009s after read data start\n",
      "5301-23.42% done, 660.838s after read data start\n",
      "5401-24.31% done, 682.962s after read data start\n",
      "5501-25.22% done, 705.526s after read data start\n",
      "5601-26.14% done, 727.861s after read data start\n",
      "5701-27.09% done, 751.001s after read data start\n",
      "5801-28.05% done, 774.831s after read data start\n",
      "5901-29.02% done, 798.434s after read data start\n",
      "6001-30.01% done, 822.625s after read data start\n",
      "6101-31.02% done, 848.992s after read data start\n",
      "6201-32.05% done, 873.757s after read data start\n",
      "6301-33.09% done, 900.844s after read data start\n",
      "6401-34.15% done, 928.235s after read data start\n",
      "6501-35.22% done, 955.109s after read data start\n",
      "6601-36.31% done, 982.832s after read data start\n",
      "6701-37.42% done, 1011.738s after read data start\n",
      "6801-38.55% done, 1040.341s after read data start\n",
      "6901-39.69% done, 1070.053s after read data start\n",
      "7001-40.85% done, 1099.205s after read data start\n",
      "7101-42.02% done, 1128.067s after read data start\n",
      "7201-43.22% done, 1159.449s after read data start\n",
      "7301-44.42% done, 1190.869s after read data start\n",
      "7401-45.65% done, 1222.487s after read data start\n",
      "7501-46.89% done, 1255.9s after read data start\n",
      "7601-48.15% done, 1290.271s after read data start\n",
      "7701-49.43% done, 1325.597s after read data start\n",
      "7801-50.72% done, 1361.211s after read data start\n",
      "7901-52.03% done, 1399.179s after read data start\n",
      "8001-53.35% done, 1436.16s after read data start\n",
      "8101-54.69% done, 1473.016s after read data start\n",
      "8201-56.05% done, 1512.916s after read data start\n",
      "8301-57.43% done, 1551.504s after read data start\n",
      "8401-58.82% done, 1593.228s after read data start\n",
      "8501-60.23% done, 1636.352s after read data start\n",
      "8601-61.65% done, 1676.93s after read data start\n",
      "8701-63.09% done, 1719.705s after read data start\n",
      "8801-64.55% done, 1761.971s after read data start\n",
      "8901-66.03% done, 1808.027s after read data start\n",
      "9001-67.52% done, 1852.653s after read data start\n",
      "9101-69.03% done, 1898.244s after read data start\n",
      "9201-70.55% done, 1942.489s after read data start\n",
      "9301-72.1% done, 1989.459s after read data start\n",
      "9401-73.66% done, 2037.198s after read data start\n",
      "9501-75.23% done, 2088.982s after read data start\n",
      "9601-76.82% done, 2138.003s after read data start\n",
      "9701-78.43% done, 2188.571s after read data start\n",
      "9801-80.06% done, 2239.164s after read data start\n",
      "9901-81.7% done, 2291.853s after read data start\n",
      "10001-83.36% done, 2345.044s after read data start\n",
      "10101-85.03% done, 2401.447s after read data start\n",
      "10201-86.72% done, 2455.888s after read data start\n",
      "10301-88.43% done, 2509.614s after read data start\n",
      "10401-90.16% done, 2568.036s after read data start\n",
      "10501-91.9% done, 2625.436s after read data start\n",
      "10601-93.66% done, 2684.806s after read data start\n",
      "10701-95.43% done, 2744.971s after read data start\n",
      "10801-97.23% done, 2808.107s after read data start\n",
      "10901-99.03% done, 2867.771s after read data start\n",
      "calculate distance done, 2898.07s after read data start\n"
     ]
    }
   ],
   "source": [
    "# dff = dff.reset_index()\n",
    "origin_trajs = dff[\"origin_trajs\"].to_list()\n",
    "arr = [np.array(origin_traj) for origin_traj in origin_trajs]\n",
    "length = len(arr)\n",
    "dis_matrix = np.zeros((length, length))\n",
    "dis_func_name = \"sspd\"\n",
    "dis_func = getattr(tdist, dis_func_name)\n",
    "\n",
    "\n",
    "def cal_dis(i, j, x, y, n):\n",
    "    dis = dis_func(x, y)\n",
    "    if i == j + 1 and i % 100 == 1:\n",
    "        timer.tok(f'{i}-{round((i * i) / (n * n) * 100, 2)}%')\n",
    "    return i, j, dis\n",
    "\n",
    "res = Parallel(n_jobs=44)(\n",
    "    delayed(cal_dis)(i, j, arr[i], arr[j], length - 1) for i in range(length) for j in range(i))\n",
    "timer.tok(\"calculate distance\")\n",
    "for (i, j, dis) in res:\n",
    "    dis_matrix[i,j] = dis\n",
    "    dis_matrix[j,i] = dis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成 Train Dataset 第六步：保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save done, 3077.513s after read data start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3077.5132780075073"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isForTrain = True\n",
    "# isForTrain = False\n",
    "\n",
    "file_name = file_path.split(\"/\")[-1]\n",
    "\n",
    "save_path = \"data/test/\"\n",
    "file_path = save_path + file_name\n",
    "origin_trajs = dff[\"origin_trajs\"].to_list()\n",
    "\n",
    "if isForTrain:\n",
    "    dict_save = {'trajs': dff[\"trajs\"].to_list(), 'origin_trajs': origin_trajs, \"dis_matrix\": dis_matrix.tolist()}\n",
    "    json.dump(dict_save, open(file_path + f\"_{len(origin_trajs)}_{vocab_size}_{dis_func_name}_dataset.json\", \"w\"))\n",
    "else:\n",
    "    df_save = df[['len','start_time','end_time','origin_trajs']]\n",
    "    df_save.to_csv(file_path + f\"_{len(origin_trajs)}_info.csv\", index=False)\n",
    "timer.tok(\"save\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71749671e259e2f388bff91d1027364dfcb1946679d787bfae26f99b00e31da0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('workspace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
